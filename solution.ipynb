{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04c530cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"no gpu\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93d9776c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1600000 texts to data/raw_dataset.csv\n",
      "Saved 1600000 texts to data/dataset_processed.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data/dataset_processed.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !!! ЗАГРУЗКА ДАТАСЕТА !!!\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.data_utils import clean_string\n",
    "from src.data_utils import save_dataset\n",
    "from src.next_token_dataset import load_sentiment140\n",
    "\n",
    "texts = load_sentiment140()\n",
    "save_dataset(texts, \"raw_dataset.csv\")\n",
    "\n",
    "# \"чистим\" тексты\n",
    "cleaned_texts = list(map(clean_string, texts))\n",
    "save_dataset(cleaned_texts, \"dataset_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12223716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1280000 texts to data/train.csv\n",
      "Saved 160000 texts to data/val.csv\n",
      "Saved 160000 texts to data/test.csv\n",
      "1280000 160000 160000\n"
     ]
    }
   ],
   "source": [
    "# !!! РАЗБИВКА НА TRAIN/VAL/TEST !!!\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train + temp (temp will become val + test)\n",
    "train_texts, temp_texts = train_test_split(\n",
    "    cleaned_texts,\n",
    "    test_size=0.2,       # 20% of data will be val+test\n",
    "    random_state=42     # for reproducibility\n",
    ")\n",
    "\n",
    "# Split temp into validation and test (50% each → 10% of total each)\n",
    "val_texts, test_texts = train_test_split(\n",
    "    temp_texts,\n",
    "    test_size=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "save_dataset(train_texts, \"train.csv\")\n",
    "save_dataset(val_texts, \"val.csv\")\n",
    "save_dataset(test_texts, \"test.csv\")\n",
    "\n",
    "# Check sizes\n",
    "print(len(train_texts), len(val_texts), len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6254f5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 12039314 texts to data/train_dataset_tokenized.csv\n",
      "Saved 1510291 texts to data/val_dataset_tokenized.csv\n",
      "train_dataset: 12039314, val_dataset: 1510291\n",
      "train_loader: 20000, val_loader: 2500\n"
     ]
    }
   ],
   "source": [
    "# !!! ТОКЕНИЗАЦИЯ И ДАТАЛОУДЕРЫ !!!\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "from src.data_utils import BertDataset\n",
    "from src.data_utils import collate_fn\n",
    "from src.next_token_dataset import TextCompletionDataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "#tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "special_tokens = ['<user>', '<url>', '<emotion>']\n",
    "tokenizer.add_tokens(special_tokens)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\") # without special chars\n",
    "transformer_tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_dataset = BertDataset(train_texts, tokenizer)\n",
    "val_dataset = BertDataset(val_texts, tokenizer)\n",
    "\n",
    "save_dataset(train_dataset.samples, \"train_dataset_tokenized.csv\")\n",
    "save_dataset(val_dataset.samples, \"val_dataset_tokenized.csv\")\n",
    "\n",
    "print(f\"train_dataset: {len(train_dataset)}, val_dataset: {len(val_dataset)}\")\n",
    "\n",
    "final_train_dataset = TextCompletionDataset(train_texts, tokenizer, mode='train')\n",
    "final_val_dataset = TextCompletionDataset(val_texts, tokenizer, mode='train')\n",
    "final_test_dataset = TextCompletionDataset(test_texts, tokenizer, mode='inference')\n",
    "\n",
    "# даталоадеры\n",
    "train_loader = DataLoader(final_train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(final_val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(final_test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"train_loader: {len(train_loader)}, val_loader: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cffd9fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13048916\n"
     ]
    }
   ],
   "source": [
    "# !!! СОЗДАНИЕ LSTM !!!\n",
    "\n",
    "from src.lstm_model import LSTMClassifier\n",
    "from src.lstm_model import count_parameters\n",
    "\n",
    "#vocab_size = tokenizer.vocab_size + len(special_tokens) \n",
    "vocab_size = len(tokenizer)\n",
    "hidden_dim = 128\n",
    "\n",
    "model = LSTMClassifier(vocab_size, hidden_dim).to(device)\n",
    "param_count = count_parameters(model)\n",
    "\n",
    "print(param_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0db7a48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [16:01<00:00, 20.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 2.939 | Val Loss: 2.278 | Val Accuracy: 64.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [16:03<00:00, 20.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 2.234 | Val Loss: 2.249 | Val Accuracy: 64.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [16:09<00:00, 20.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 2.203 | Val Loss: 2.239 | Val Accuracy: 64.46%\n"
     ]
    }
   ],
   "source": [
    "# !!! ОБУЧЕНИЕ LSTM !!!\n",
    "\n",
    "from src.eval_lstm import evaluate\n",
    "from src.lstm_train import train\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "train(model, train_loader, val_loader, criterion, optimizer, evaluate, tokenizer.pad_token_id, device, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6718c06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:   4%|▎         | 5964/160000 [00:12<05:33, 462.50it/s]"
     ]
    }
   ],
   "source": [
    "from src.eval_transformer_pipeline import evaluate_rouge\n",
    "\n",
    "# !!! ROUGE-ОЦЕНКА LSTM !!!\n",
    "predictionsLSTM = []\n",
    "referencesLSTM = []\n",
    "\n",
    "for batch in tqdm(test_loader, desc=\"Generating\"):\n",
    "    input_ids = batch[\"input_ids\"][0]  # один sample\n",
    "    target_ids = batch[\"target_ids\"][0]\n",
    "\n",
    "    gen_text = model.generate(input_ids, max_new_tokens=len(target_ids), tokenizer=tokenizer)\n",
    "\n",
    "    target_text = tokenizer.decode(target_ids, skip_special_tokens=True)\n",
    "\n",
    "    predictionsLSTM.append(gen_text)\n",
    "    referencesLSTM.append(target_text)\n",
    "\n",
    "evaluate_rouge(predictions=predictionsLSTM, references=referencesLSTM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c092d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Generating:   0%|          | 10/5000 [00:00<03:30, 23.65sample/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Generating:   1%|          | 27/5000 [00:00<01:52, 44.20sample/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Generating: 100%|██████████| 5000/5000 [01:43<00:00, 48.16sample/s]\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# !!! РАБОТА ТРАНСФОРМЕРА !!!\n",
    "\n",
    "from transformers import pipeline\n",
    "from src.eval_transformer_pipeline import evaluate_rouge\n",
    "from tqdm import tqdm\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"distilgpt2\",\n",
    "    tokenizer=transformer_tokenizer,\n",
    "    device=0               # или -1 для CPU\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for sample in tqdm(final_test_dataset, desc=\"Generating\", unit=\"sample\"):\n",
    "    input_text = tokenizer.decode(\n",
    "        sample[\"input_ids\"],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    target_text = tokenizer.decode(\n",
    "        sample[\"target_ids\"],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    out = generator(\n",
    "        input_text,\n",
    "        max_new_tokens=len(sample[\"target_ids\"]),\n",
    "        do_sample=False,\n",
    "        temperature=None\n",
    "    )\n",
    "\n",
    "    gen_text = out[0][\"generated_text\"]\n",
    "    gen_completion = gen_text[len(input_text):].strip()\n",
    "\n",
    "    #print(f\"input_text: {input_text}, target_text: {target_text}, out: {gen_completion}\")\n",
    "\n",
    "    predictions.append(gen_completion)\n",
    "    references.append(target_text)\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4601bae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics\n",
      "rouge1: 0.0687\n",
      "rouge2: 0.0105\n",
      "rougeL: 0.0681\n",
      "rougeLsum: 0.0681\n",
      "Examples:\n",
      "ref:  internet for april and may, prediction: texts.”\n",
      "ref:  of hoarding out of apartment, prediction: of the money.\n",
      "ref: s not feeling up to par, prediction: was a girl she would be\n",
      "ref:  hate smiling, prediction: can't\n",
      "ref:  good friend riley, prediction: good person.\n",
      "ref:  need to mention that sorry, prediction: know i was going to\n"
     ]
    }
   ],
   "source": [
    "# !!! ROUGE-ОЦЕНКА ТРАНСФОРМЕРА !!!\n",
    "\n",
    "evaluate_rouge(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f35c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! ВЫВОДЫ !!!\n",
    "\n",
    "# - предобученный трансформер GPT2 в метриках ROUGE показал чуть более лучший результат. Rouge1: LSTM: 0.0468 vs Трансформер: 0.0687\n",
    "# - модель LSTM на каждой следующей эпохе обучения немного улучшает точность предсказаний. При этом она не переобучается: значения train loss и val loss схожи.\n",
    "#       Epoch 1 | Train Loss: 2.939 | Val Loss: 2.278 | Val Accuracy: 64.04%\n",
    "#       Epoch 2 | Train Loss: 2.234 | Val Loss: 2.249 | Val Accuracy: 64.33%\n",
    "#       Epoch 3 | Train Loss: 2.203 | Val Loss: 2.239 | Val Accuracy: 64.46%\n",
    "# - ручная проверка осмысленности автодополнений на выборочных примерах вынуждает принять решение в пользу использования трансформера gpt2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
